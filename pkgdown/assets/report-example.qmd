---
title: MetaCheck Report
subtitle: "To Err is Human: An Empirical Investigation"
metacheck:
  version: "0.0.0.9063"
  report-created: "2025-12-23"
format:
  html:
    theme: 
      light: flatly
      dark: darkly
    respect-user-color-scheme: true
    toc: true
    toc-title: >
      <ul>
        <li class="green">no problems detected;</li>  
        <li class="yellow">something to check;</li> 
        <li class="red">possible problems detected;</li>  
        <li class="info">informational only;</li>  
        <li class="na">not applicable;</li>  
        <li class="fail">check failed</li>
      <ul>
      <hr/>
      **Table of Contents**
    toc-location: right
    toc-expand: 3
    page-layout: full
    title-block-banner: "#327118"
    title-block-banner-color: "white"
    link-external-newwindow: true
    embed-resources: true
    format-links: false
execute: 
  echo: false
  error: true
---

<style>
  main h2, main h3 { margin-left: -20px; }
  main h3 { font-size: 125%; }
  main h4 { font-size: 110%; }
  .na::before     { content: '‚ö™ '; } /* ‚ö™ */
  .fail::before   { content: '‚ò†Ô∏è '; } /* ‚ö´Ô∏è */
  .info::before   { content: '‚ÑπÔ∏è '; } /* üîµ */
  .red::before    { content: '‚ö†Ô∏è '; } /* üî¥ */
  .yellow::before { content: 'Ô∏èüîç '; } /* üü° */
  .green::before  { content: '‚úÖ '; } /* üü¢ */
  section::before { content: '' !important; }
  th { background-color: #555; color: white; }
  table.dataTable, .dataTables_scrollBody { margin: 0.5em 0; }
  #info { position: absolute; top: 2.5em; right: 2.5em; color: white; }
  #info a { color: white !important; }
  .quarto-dark tr.even { background-color: #333; }
  .quarto-dark tr.odd { background-color: #222; }
  .quarto-dark div.datatables { color: white; }
  .quarto-dark table.dataTable { border: 1px solid #555; }
</style>

::: {#info}
[MetaCheck](http://www.scienceverse.org/metacheck) version {{< meta metacheck.version >}}  
Report Created: {{< meta metacheck.report-created >}}  
 <!-- doi, if exists -->
:::

[Metacheck](https://www.scienceverse.org/metacheck/) is a tool that screens scientific manuscripts and aims to identify potential issues for improvement, thereby guiding researchers towards best practices. Metacheck is developed to help researchers correctly and completely report statistical results, automatically retrieve possible relevant information about citations, and improve how researchers share data, code, and preregistrations.

::: {.callout-tip title="Learn More" collapse="true"}

Metacheck combines existing and new checks in a module-based tool. It mainly relies on text search, retrieving information from external sources through API‚Äôs or web-scraping, but it also incorporates tool that use machine learning classifiers or large language models. The use of LLM‚Äôs is always optional. The development of Metacheck is guided by our [values statement](https://docs.google.com/document/d/1bbIkgUaiz3fpTXAeTF3h-gsfhqWEBN_n6OBcbFLfXjw/edit?tab=t.0). 

Metacheck often needs to balance false positives against false negatives, and prioritizes preventing false negatives. Like a spelling checker, which often highlights words that are not spelled incorrectly, Metacheck modules will contain false positives. We hope their rate is acceptable, given opportunities for improvement that Metacheck identifies. Modules are currently primarily validated on the psychological literature.

Metacheck is under continuous development. Issues can be submitted [on Github](https://github.com/scienceverse/metacheck/issues), and suggestions for improvement or feedback can be sent to D.Lakens@tue.nl.

:::


## Summary

- [Open Practices Check](#open-practices-check){.green}: Data and code are shared.  
- [Preregistration Check](#preregistration-check){.info}: We found 2 preregistrations.  
- [Randomization and Causal Claims](#randomization-and-causal-claims){.green}: 
    -  We identified 1 sentence describing randomization.
    -  No causal claims were observed in the title.
    -  No causal claims were observed in the abstract.  
- [Power](#power){.red}: You included power analysis, but some essential reporting aspects appear missing.  
- [Exact P-Values](#exact-p-values){.red}: We found 1 imprecise *p* value out of 3 detected.  
- [Non-Significant P Value Check](#non-significant-p-value-check){.yellow}: We found 2 non-significant p values that should be checked for appropriate interpretation.  
- [Marginal Significance](#marginal-significance){.red}: You described 2 effects with terms related to 'marginally significant'.  
- [Effect Sizes in t-tests and F-tests](#effect-sizes-in-t-tests-and-f-tests){.red}: We found 1 t-test and/or F-test where effect sizes are not reported.  
- [StatCheck](#statcheck){.red}: 1 possible error in t-tests or F-tests  
- [Code Check](#code-check){.info}: 
    -  Some files loaded in the R scripts were missing in the repository.
    -  Hardcoded file paths were found.
    -  Libraries were loaded in multiple places.
    -  Some code files had no comments.
    -  No README file was found.  
- [DOI Check](#doi-check){.yellow}: We checked 1 reference in CrossRef and found 1 missing DOI  
- [Reference Accuracy](#reference-accuracy){.yellow}: We checked 3 references with DOIs in CrossRef and found  matches for 2.  
- [Replication Check](#replication-check){.info}: You cited 1 article in the FReD replication database.  
- [RetractionWatch](#retractionwatch){.info}: You cited 1 article in the RetractionWatch database.  
- [Check PubPeer Comments](#check-pubpeer-comments){.info}: You cited 1 reference with comments in PubPeer.  



## General Modules

### Open Practices Check {.green}

Data was openly shared for this article, based on the sentence "Data is also available from <https://osf.io/5tbm9> and code is also available from <https://osf.io/629bx>.".

Code was openly shared for this article, based on the sentence "Data and analysis code is available on GitHub from <https://github.com/Lakens/to_err_is_human> and from <https://researchbox.org/4377>.".

::: {.callout-tip title="Learn More" collapse="true"}

Data and code sharing was determined using ODDPub, a text mining algorithm.

Riedel N, Kip M, Bobrov E (2020). &ldquo;ODDPub ‚Äì a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications.&rdquo; <em>Data Science Journal</em>, <b>19</b>(1), 42. <a href="https://doi.org/10.5334/dsj-2020-042">doi:10.5334/dsj-2020-042</a>.

:::

::: {.callout-note title="How It Works" collapse="true"}

This module incorporates ODDPub into metacheck. ODDPub is a text mining algorithm that detects which publications disseminated Open Data or Open Code together with the publication.

The Open Practices Check runs Open Data Detection in Publications (ODDPub). ODDPub searches for regular expressions that match a predefined pattern, and identifies sentences describing open practices such as code and data sharing. More information on the package can be found at <https://github.com/quest-bih/oddpub>. The module only returns whether open data and code is found (the original package offers more fine-grained results). The tool was validated in the biomedical literature, see <https://osf.io/yv5rx/>.

ODDPub was developed by Nico Riedel, Vladislav Nachev, Miriam Kip, and Evgeny Bobrov at the QUEST Center for Transforming Biomedical Research, Berlin Institute of Health.

It might miss open data and code declarations when the words used in the manuscript are not in the pattern that ODDPub searches for.

This module was developed by Daniel Lakens

:::

## Method Modules

### Preregistration Check {.info}

We found 2 preregistrations.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(id = c("<a href='https://osf.io/48ncu' target='_blank'>48ncu</a>", 
"<a href='https://aspredicted.org/by8i8v.pdf' target='_blank'>by8i8v</a>"
), title = c("Papercheck Test", "To err is human"), template = c("OSF Preregistration", 
"AsPredicted")), class = "data.frame", row.names = c(NA, -2L))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

Meta-scientific research has shown that deviations from preregistrations are often not reported or checked, and that the most common deviations concern the sample size. We recommend manually checking the full preregistration at the links below, and have provided the preregistered sample size.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(id = c("48ncu", "by8i8v"), sample_size = c("We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Full Preregistration" collapse="true"}

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Field = c("template_name", "title", "id", "link", 
"date_created", "date_modified", "date_registered", "embargo_end_date", 
"ia_url", "description", "study_type", "blinding", "study_design_overview", 
"data_collection_started", "existing_data_explanation", "data_collection_procedures", 
"sample_size", "sample_size_rationale", "stopping_rule", "design_independent_variables", 
"design_dependent_variables", "indices", "statistical_tests", 
"inference_criteria", "data_exclusion_criteria", "outliers_and_exclusions", 
"exploratory_analyses", "additional_comments", "research_questions"
), "Preregistration 1" = c("OSF Preregistration", "Papercheck Test", 
"48ncu", "https://osf.io/48ncu", "2025-11-28T19:14:24.639665", 
"2025-08-05T11:55:35.267689", "2025-11-28T19:14:24.608826", "NA", 
"https://archive.org/details/osf-registrations-48ncu-v1", "Automation can be used to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of metacheck to improve best practices.", 
"Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.", 
"No blinding is involved in this study. ", "Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
"Registration prior to creation of data", "", "We will include all scientists who are willing to participate. ", 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
"Based on an a-priori power analysis for an independent t-test, with an expected effect size of d = 0.6, an alpha of 0.05, and a desired power of 0.85, we needed to collect at least 50 participants per group. ", 
"We will stop after data from 100 researchers has been collected.", 
"Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
"We will code all manuscripts for mistakes, and count the total number of mistakes per manuscript.", 
"", "We will perform an independent t-test on the number of mistakes in each group.", 
"We will interpret effects as significant if p &lt; .05", "No exclusions are expected.", 
"No missing data is expected.\n", "We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful).", 
"", NA), "Preregistration 2" = c("AsPredicted", "To err is human", 
"by8i8v", "https://aspredicted.org/by8i8v.pdf", "2025/11/27 23:20 (PT)", 
NA, NA, NA, NA, NA, NA, NA, "Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
NA, "No, no data have been collected for this study yet.", NA, 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
NA, NA, NA, "We will code all manuscripts for mistakes, and count the total number of mistakes per manuscript.", 
NA, "We will perform an independent t-test on the number of mistakes in each group.", 
NA, NA, "none are expected.", NA, "We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful).", 
"Automation can be used to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of metacheck to improve best practices."
)), class = "data.frame", row.names = c("template_name", "title", 
"id", "link", "date_created", "date_modified", "date_registered", 
"embargo_end_date", "ia_url", "description", "study_type", "blinding", 
"study_design_overview", "data_collection_started", "existing_data_explanation", 
"data_collection_procedures", "sample_size", "sample_size_rationale", 
"stopping_rule", "design_independent_variables", "design_dependent_variables", 
"indices", "statistical_tests", "inference_criteria", "data_exclusion_criteria", 
"outliers_and_exclusions", "exploratory_analyses", "additional_comments", 
"research_questions"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "<'top' p>", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

:::

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate of deviationsfrom preregistrations, see:

van den Akker O, Bakker M, van Assen M, Pennington C, Verweij L, Elsherif M, Claesen A, Gaillard S, Yeung S, Frankenberger J, Krautter K, Cockcroft J, Kreuer K, Evans T, Heppel F, Schoch S, Korbmacher M, Yamada Y, Albayrak-Aydemir N, Wicherts J (2024). &ldquo;The potential of preregistration in psychology: Assessing preregistration producibility and preregistration-study consistency.&rdquo; <em>Psychological Methods</em>. <a href="https://doi.org/10.1037/met0000687">doi:10.1037/met0000687</a>.

For educational material on how to report deviations from preregistrations, see:

Lakens, Dani√´l (2024). &ldquo;When and How to Deviate From a Preregistration.&rdquo; <em>Collabra: Psychology</em>, <b>10</b>(1), 117094. <a href="https://doi.org/10.1525/collabra.117094">doi:10.1525/collabra.117094</a>.

:::

::: {.callout-note title="How It Works" collapse="true"}

Retrieve information from preregistrations in a standardised way,
and make them easier to check.

The Preregistration Check module identifies preregistrations on the OSF and AsPredicted based on links in the manuscript, retrieves the preregistration text, and organizes the information into a template. The module then uses regular expressions to identify text from AsPredicted, and the API to retrieve text from the OSF. The information in the preregistration is returned.

The module can‚Äôt extract information from non-structured preregistration templates (i.e., where the preregistration is uploaded in a single text field) and it can‚Äôt retrieve information in preregistrations that are stored as text documents on the OSF.

If you want to extend the package to be able to download information from other preregistration sites, reach out to the Metacheck development team.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### Randomization and Causal Claims {.green}

Journal Article Reporting Standards require details about randomization procedures, or how possible bias due to non-randomization is mitigated. This information is often not reported. Furthermore, researchers sometimes make causal claims that are not warranted, for example because there was no random assignment to conditions. This module checks how (non)randomization is reported, and checks for causal claims in the title and abstract. Researchers are asked to double check whether this information is reported completely and correctly.

#### Randomization

We identified 1 sentence describing randomization.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

If this was a study that contained random assignment to conditions, the journal article reporting standards (JARS) ask that you describe the following:

1. Random assignment method: Procedure used to generate the random assignment sequence, including details of any restriction (e.g., blocking, stratification)

2. Random assignment concealment: Whether sequence was concealed until interventions were assigned

3. Random assignment implementation: Who generated the assignment sequence, who enrolled participants, who assigned participants to groups

#### Causal Claims

No causal claims were observed in the title.

No causal claims were observed in the abstract.

::: {.callout-tip title="Learn More" collapse="true"}

For advice on how to make causal claims, and when not to, see:

Antonakis J, Bendahan S, Jacquart P, Lalive R (2010). &ldquo;On making causal claims: A review and recommendations.&rdquo; <em>The Leadership Quarterly</em>, <b>21</b>(6), 1086&ndash;1120. <a href="https://doi.org/10.1016/j.leaqua.2010.10.010">doi:10.1016/j.leaqua.2010.10.010</a>.

Grosz M, Rohrer J, Thoemmes F (2020). &ldquo;The Taboo Against Explicit Causal Inference in Nonexperimental Psychology.&rdquo; <em>Perspectives on Psychological Science</em>, <b>15</b>(5), 1243&ndash;1255. <a href="https://doi.org/10.1177/1745691620921521">doi:10.1177/1745691620921521</a>.

For the APA journal articles reporting standards, see <https://apastyle.apa.org/jars>

:::

::: {.callout-note title="How It Works" collapse="true"}

Aims to identify the presence of random assignment, and lists sentences that make causal claims in title or abstract.

The Randomization and Causal Claims Check first uses regular expressions to check whether the manuscript contains a statement about randomization to conditions. Subsequently, it sends the title and abstract to a [machine learning classifier developed by Rasoul Norouzi](https://github.com/rasoulnorouzi/causal_relation_miner) that runs on [HuggingFace](https://huggingface.co/spaces/lakens/causal_sentences). Causal statements are identified. Researchers are recommended to double check if causal statements are warranted, especially if no sentences describing randomization were detected.

The regular expressions can miss statements about randomization, or incorrectly assume there is a sentence describing randomization. The module can‚Äôt evaluate if the causal statements that are identified are warranted or not, and it only reminds users to double-check.

If you want to improve the detection of sentences describing randomization, or otherwise improve the module, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

### Power {.red}

You included power analysis, but some essential reporting aspects appear missing.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("We conducted a sensitivity power analysis to determine that a Cohen's d of 0.50 is the smallest effect size that we could detect with 50 participants in each group and 80% power."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 1, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

Power analyses need to contain the following information to be interpretable: the statistical test, sample size, critical alpha criterion, power level, effect size, and an effect size metric. For example:

> An a priori power analysis for an independent samples t-test, conducted using the pwr.t.test function from pwr (Champely, 2020) with Cohen's d = 0.5 and a critical alpha of p = 0.05, determined that 64 participants are required in each group for 80% power.

## Results Modules

### Exact P-Values {.red}

We found 1 imprecise *p* value out of 3 detected. Reporting *p* values imprecisely (e.g., *p* < .05) reduces transparency, reproducibility, and re-use (e.g., in *p* value meta-analyses). Best practice is to report exact p-values with three decimal places (e.g., *p* = .032) unless *p* values are smaller than 0.001, in which case you can use *p* < .001.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("P-Value" = "p > .05", Sentence = "There was no effect of experience on the reduction in errors when using the tool (p > .05), as the correlation was non-significant."), row.names = c(NA, 
-1L), class = c("tbl_df", "tbl", "data.frame"))

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "10%"), list(targets = 1, width = "90%"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

The APA manual states: Report exact *p* values (e.g., *p* = .031) to two or three decimal places. However, report *p* values less than .001 as *p* < .001. However, 2 decimals is too imprecise for many use-cases (e.g., a *p* value meta-analysis), so report *p* values with three digits.

American Psychological Association (2020). <em>Publication manual of the American Psychological Association</em>, 7 edition. American Psychological Association.

:::

::: {.callout-note title="How It Works" collapse="true"}

List any p-values reported with insufficient precision (e.g., p < .05 or p = n.s.)

This module was developed by Lisa DeBruine

:::

### Non-Significant P Value Check {.yellow}

We found 2 non-significant p values that should be checked for appropriate interpretation.

Meta-scientific research has shown nonsignificant p values are commonly misinterpreted. It is incorrect to infer that there is 'no effect', 'no difference', or that groups are 'the same' after p > 0.05.

It is possible that there is a true non-zero effect, but that the study did not detect it. Make sure your inference acknowledges that it is possible that there is a non-zero effect. It is correct to include the effect is 'not significantly' different, although this just restates that p > 0.05.

Metacheck does not yet analyze automatically whether sentences which include non-significant p-values are correct, but we recommend manually checking the sentences below for possible misinterpreted non-significant p values.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Text = c("p = 0.152", "p > .05"), Sentence = c("On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152.", 
"There was no effect of experience on the reduction in errors when using the tool (p > .05), as the correlation was non-significant."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "10%"), list(targets = 1, width = "90%"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate of misinterpretations of non-significant results is high, see:

Aczel B, Palfi B, Szollosi A, Kovacs M, Szaszi B, Szecsi P, Zrubka M, Gronau Q, van den Bergh D, Wagenmakers E (2018). &ldquo;Quantifying Support for the Null Hypothesis in Psychology: An Empirical Investigation.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>(3), 357&ndash;366. <a href="https://doi.org/10.1177/2515245918773742">doi:10.1177/2515245918773742</a>.

Murphy S, Merz R, Reimann L, Fern√°ndez A (2025). &ldquo;Nonsignificance misinterpreted as an effect‚Äôs absence in psychology: Prevalence and temporal analyses.&rdquo; <em>Royal Society Open Science</em>, <b>12</b>(3), 242167. <a href="https://doi.org/10.1098/rsos.242167">doi:10.1098/rsos.242167</a>.

For educational material on preventing the misinterpretation of p values, see [Improving Your Statistical Inferences](https://lakens.github.io/statistical_inferences/01-pvalue.html#sec-misconception1).

:::

::: {.callout-note title="How It Works" collapse="true"}

This module checks for imprecisely reported p values. If p > .05 is detected, it warns for misinterpretations.

The nonsignificant p-value check searches for regular expressions that match a predefined pattern. The module identifies all p-values in a manuscript and selects those that are not reported to be smaller than or equal to 0.05. It returns all sentences containing non-significant p-values.

In the future, the Metacheck team aims to incorporate a machine learning classifier to only return sentences likely to contain misinterpretations. If you want to help to improve the module, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

### Marginal Significance {.red}

You described effects with terms related to 'marginally significant'. If *p* values above 0.05 are interpreted as an effect, you inflate the alpha level, and increase the Type 1 error rate. If a *p* value is higher than the prespecified alpha level, it should be interpreted as a non-significant result.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(section = c("abstract", "results"), text = c("The paper shows examples of (1) open and closed OSF links; (2a) citation of retracted papers, (2b) citations without a doi, (2c) citations with Pubpeer comments, (2d) citations in the FORTT replication database, and (2e) missing/mismatched/incorrect citations and references; (3a) R files with code on GitHub that do not load libraries in one location, (3b) load files that are not shared in the repository, (3c) lack comments, and (3d) have hard-coded files, (4) imprecise reporting of non-significant pvalues; (5) tests with and without effect sizes, (6) use of \"marginally significant\" to describe non-significant findings, and (7) retrieving information from preregistrations.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate at which non-significant p-values are interpreted as marginally significant, see:

Olsson-Collentine, A., van Assen, M. MAL, Hartgerink &amp;, J. CH (2019). &ldquo;The Prevalence of Marginally Significant Results in Psychology Over Time.&rdquo; <em>Psychological Science</em>, <b>30</b>, 576&ndash;586. <a href="https://doi.org/10.1177/0956797619830326">doi:10.1177/0956797619830326</a>.

For the list of terms used to identifify marginally significant results, see this [blog post by Matthew Hankins](https://web.archive.org/web/20251001114321/https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/).

:::

::: {.callout-note title="How It Works" collapse="true"}

List all sentences that describe an effect as 'marginally significant'.

The marginal module searches for regular expressions that match a predefined pattern. The list of terms is a subset of those listed in a [blog post by Matthew Hankins](https://web.archive.org/web/20251001114321/https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/). The module returns all sentences that match terms describing ‚Äòmarginally significant‚Äô results.

Some of the terms identified might not be problematic in some contexts, and there are ways to describe ‚Äòmarginal significance‚Äô that are not detected by the module.

This module was developed by Daniel Lakens

:::

### Effect Sizes in t-tests and F-tests {.red}

We found 1 t-test and/or F-test where effect sizes are not reported. We recommend checking the sentences below, and add any missing effect sizes.

The following sentences are missing effect sizes

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating that effect sizes are often not reported:

* Peng, C.-Y. J., Chen, L.-T., Chiang, H.-M., & Chiang, Y.-C. (2013). The Impact of APA and AERA Guidelines on Effect Size Reporting. Educational Psychology Review, 25(2), 157‚Äì209. doi:[10.1007/s10648-013-9218-2](https://doi.org/10.1007/s10648-013-9218-2).

For educational material on reporting effect sizes:

* [Guide to Effect Sizes and Confidence Intervals](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals/)

:::

::: {.callout-tip title="All detected and assessed stats" collapse="true"}

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Sentence = c("On average researchers in the experimental (app) condition made fewer mistakes (M = 9.12) than researchers in the control (checklist) condition (M = 10.9), t(97.7) = 2.9, p = 0.005, d = 0.59.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
), Section = c("method", "results"), "Effect Size" = c("d = 0.59", 
NA), "Reported Test" = c("t(97.7) = 2.9", "t(97.2) = -1.96"), 
    "Test Type" = c("t-test", "t-test")), row.names = c(NA, -2L
), class = c("tbl_df", "tbl", "data.frame"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

:::

::: {.callout-note title="How It Works" collapse="true"}

The Effect Size module checks for effect sizes in t-tests and F-tests.

The Effect Size check searches for regular expressions that match a predefined pattern. The module was validated on APA reported statistical tests, and might miss effect sizes that were reported in other reporting styles. It was validated by the Metacheck team on papers published in Psychological Science.

If you want to extend the package to detect effect sizes for additional tests, reach out to the Metacheck development team.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### StatCheck {.red}

We detected possible errors in test statistics. Note that as the accuracy of statcheck has only been validated for *t*-tests and *F*-tests. As Metacheck only uses validated modules, we only provide statcheck results for *t* tests and *F*-tests.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Text = "t(97.2) = -1.96, p = 0.152", "Recomputed p" = 0.05286, 
    Section = "results", Sentence = "On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."), row.names = 2L, class = c("statcheck", 
"data.frame"))

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "10em"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific research on the validity of statcheck, and it's usefulness to prevent statistical reporting errors, see:

Nuijten M, van Assen M, Hartgerink C, Epskamp S, Wicherts J (2017). &ldquo;The validity of the tool &quot;statcheck&quot; in discovering statistical reporting inconsistencies.&rdquo; <a href="https://doi.org/10.31234/osf.io/tcxaja">doi:10.31234/osf.io/tcxaja</a>, Preprint.

Nuijten M, Wicherts J (2023). &ldquo;The effectiveness of implementing statcheck in the peer review process to avoid statistical reporting errors.&rdquo; <a href="https://doi.org/10.31234/osf.io/bxau9">doi:10.31234/osf.io/bxau9</a>, Preprint.

:::

::: {.callout-note title="How It Works" collapse="true"}

Check consistency of p-values and test statistics

The Statcheck module runs Statcheck. Statcheck searches for regular expressions that match a predefined pattern, and identifies APA reported statistical tests. More information on the package can be found at <https://github.com/cran/statcheck>. The module only returns Statcheck results for t-tests and F-tests, as these are the only tests which have been validated, see <https://osf.io/preprints/psyarxiv/tcxaj_v1/>.

Statcheck was developed by Mich√®le Nuijten and Sascha Epskamp.

Statcheck considers p = 0.000 an error, as you should report p < 0.001. Furthermore, p < 0.03 is an error if the p-value was 0.031, and one should simply report exact p-values (p = 0.031). Statcheck might miss one-sided tests, and falsely assume the p-value is incorrect. For more information, see [StatCheck](https://statcheck.io/).

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### Code Check {.info}

Below, we describe some best coding practices and give the results of automatic evaluation of these practices in the R files below. This check may miss things or produce false positives if your R scripts are less typical.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("R File Name" = c("bad.R", "bad.Rmd", "analysis.R"
), Download = c("<a href='https://osf.io/download/6935bc117049c887a125cb25/'>download</a>", 
"<a href='https://osf.io/download/uq347/'>download</a>", "<a href='https://raw.githubusercontent.com/Lakens/to_err_is_human/main/analysis.R'>download</a>"
)), row.names = c(NA, -3L), class = c("tbl_df", "tbl", "data.frame"
))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

#### Missing Files

The scripts load files, but 3 scripts loaded files that could not be automatically identified in the repository. Check if the following files are made available, so that others can reproduce your code, or that the files are missing:

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("R File names" = c("bad.R", "bad.Rmd", "analysis.R"
), "Files loaded in R file but missing in repository" = c("example.csv", 
"example.csv", "file.csv")), row.names = c(NA, -3L), class = c("tbl_df", 
"tbl", "data.frame"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

#### Hardcoded Paths

Best programming practice is to use relative file paths instead of hardcoded file paths (e.g., C://Lakens/files) as these folder names are do not exist on other computers. The following hardcoded file paths were found in 3 R file(s).

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("R File names" = c("bad.R", "bad.Rmd", "analysis.R"
), "Absolute paths found" = c("data <- read_csv(\"C://Lakens/files/example.csv\")", 
"data <- read_csv(\"C://Lakens/files/example.csv\"), data2 <- read_csv(\"C://Lakens/files/example2.csv\")", 
"data_1 <- read.csv(\"C:/path/to/a/file.csv\")")), row.names = c(NA, 
-3L), class = c("tbl_df", "tbl", "data.frame"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

#### Libraries

Best programming practice is to load all required libraries at one place in the code. In 3 R files, libraries were at multiple places in the R files (i.e., with more than 3 lines in between). This was true in the following R files, where libraries were loaded on the following lines:

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("R File names" = c("bad.R", "bad.Rmd", "analysis.R"
), "Lines at which libraries are loaded" = c("1, 16", "9, 38", 
"1, 16")), row.names = c(NA, -3L), class = c("tbl_df", "tbl", 
"data.frame"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

#### Code Comments

Best programming practice is to add comments to code, to explain what the code does (to yourself in the future, or peers who want to re-use your code. The following 1 files had no comments:

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("R File names" = c("bad.R", "bad.Rmd", "analysis.R"
), "Percentage of lines that are comments" = c(0.0526315789473684, 
0.0222222222222222, 0)), row.names = c(NA, -3L), class = c("tbl_df", 
"tbl", "data.frame"))

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 5, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

#### README

No README file was found. README files are a way to document the contents and structure of a folder, helping users locate the information they need. You can use a README to document changes to a repository, and explain how files are named. Please consider adding a README.

::: {.callout-note title="How It Works" collapse="true"}

Retrieve information from repositories about r files, zip files, and readme.

The Code Check module lists files on the OSF and GitHub based on links in the manuscript, and retrieves R, Rmd, and Qmd files. The module then uses regular expressions to check the code. The regular expression search will detect the number of comments, the lines at which libraries are loaded, attempts to detect absolute paths to files, and lists files that are loaded, and checks if these files are in the repository. It will also check for a readme file in the repository, and will warn it can‚Äôt examine the contents of zip files. The module will return suggestions to improve the code if there are no comments, if libraries are loaded in lines further than 4 lines apart, if files that are loaded are not in the repository, and if hardcoded file paths are found.

The regular expressions can miss information in code files, or falsely detect parts of the code as a fixed file path. Libraries might be loaded in one block, even if there are more than 4 intermittent lines. The package was validated internally on papers published in Psychological Science. There might be valid reasons why some loaded files can‚Äôt be shared, but the module can‚Äôt evaluate these reasons, and always gives a warning.

If you want to extend the package to be able to download files from additional data repositories, or perform additional checks on code files, or make the checks work on other types of code files, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

## Reference Modules

### DOI Check {.yellow}

We checked 1 reference in CrossRef and found 1 missing DOI

Double check any references listed in the tables below. The match score gives an indication of how good the match was. Many books do not have a DOI or are not listed in CrossRef. Garbled references are usually a result of poor parsing of the paper by grobid; we are working on more accurate alternatives.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("Found DOI" = "<a href='https://doi.org/10.1177/2515245918770963' target='_blank'>doi.org/10.1177/2515245918770963</a>", 
    "Match Score" = 79, "Original Reference" = "Lakens D (2018). &ldquo;Equivalence testing for psychological research.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>, 259-270."), row.names = 1L, class = "data.frame")

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-note title="How It Works" collapse="true"}

This module checks references for missing DOIs.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### Reference Accuracy {.yellow}

We checked 3 references with DOIs in CrossRef and found  matches for 2.

Double check any references listed in the tables below. This tool has a high false positive rate. Mismatches may be because of problems with our parsing of references from your PDF (we're working on improving this), incorrect formatting in CrossRef, or minor differences in punctuation.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("Unfound Reference" = "Smith F (2021). &ldquo;Human error is a symptom of a poor design.&rdquo; <em>Journal of Journals</em>, <b>0</b>(0), 0. <a href=\"https://doi.org/10.0000/0123456789\">doi:10.0000/0123456789</a>."), row.names = 3L, class = "data.frame")

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "5px"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list("Original Title" = "Evil Genius? How Dishonesty Can Lead to Greater Creativity", 
    "CrossRef Title" = "Retracted: Evil Genius? How Dishonesty Can Lead to Greater Creativity", 
    Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>."), class = "data.frame", row.names = 2L)

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "5px"), list(targets = 1, width = "5px"), 
    list(targets = 2, width = "5px"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-note title="How It Works" collapse="true"}

This module checks references for mismatches with CrossRef.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### Replication Check {.info}

We checked 4 references with DOIs. You cited 1 article in the FReD replication database.

Check if you are aware of the replication studies, and cite them where appropriate.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Reference = "Gangestad SW, Thornhill R (1998). &ldquo;Menstrual cycle variation in women's preferences for the scent of symmetrical men.&rdquo; <em>Proceedings Biological Sciences</em>, <b>22</b>, 927-933. <a href=\"https://doi.org/10.1098/rspb.1998.0380\">doi:10.1098/rspb.1998.0380</a>.", 
    "Replication Reference" = "Jones, B. C., Hahn, A. C., Fisher, C. I., Wang, H., Kandrik, M., Han, C., Fasolt, V., Morrison, D., Lee, A. J., Holzleitner, I. J., O‚ÄôShea, K. J., Roberts, S. C., Little, A. C., & DeBruine, L. M. (2018). No Compelling Evidence that Preferences for Facial Masculinity Track Changes in Women‚Äôs Hormonal Status. Psychological Science, 29(6), 996-1005. https://doi.org/10.1177/0956797618760197 <a href='https://doi.org/10.1177/0956797618760197' target='_blank'>doi.org/10.1177/0956797618760197</a>"), row.names = c(NA, 
-1L), class = "data.frame")

# table setup -----------------------------------
cd <- list(list(targets = 0, width = "50%"), list(targets = 1, width = "50%"))
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations of original studies for which replication studies exist in the Replication Database.

The Replication Check module compares the reference list against studies in the FORRT replication database based on the DOI. If a study in the database is found, a reminder is provided that a replication of the original study exists, and should be cited (currently, a warning is provided regardless of whether the replication study is already cited).

The module requires that the reference has a DOI. If you run the ref_doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

It is possible the original study was cited for other reasons than the empirical claim tested, or that the replication in the FORRT replication database is for only one of the studies in the paper, and not the study the authors discuss.

The database can be manually updated with the `FReD_update()`` function. For more information, see <https://forrt.org/FReD/>.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### RetractionWatch {.info}

We checked 4 references with DOIs. You cited 1 article in the RetractionWatch database.

Check if you are aware of the replication studies, and cite them where appropriate.

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>.", 
    "RW Type" = "Retraction"), row.names = c(NA, -1L), class = "data.frame")

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations in the RetractionWatch Database.

The RetractionWatch Check module compares the reference list against studies in the RetractionWatch database based on the DOI. If a study in the database is found, a reminder is provided that the study was retracted, has an expression of concern, or a correction.

The module requires that the reference has a DOI. If you run the ref_doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

It is possible the authors are already aware that a study was retracted, but the module can't evaluate this.

The database can be manually updated with the rw_update function. For more information, see https://gitlab.com/crossref/retraction-watch-data.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### Check PubPeer Comments {.info}

We checked 4 references with DOIs. You cited 1 reference with comments in PubPeer.

Pubpeer is a platform for post-publication peer review. We have filtered out Pubpeer comments by 'Statcheck'. You can check out the comments by visiting the URLs below:

```{r}
#| echo: false

# table data ------------------------------------
table <- structure(list(Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>.", 
    Comments = 3, "PubPeer Link" = "<a href='https://pubpeer.com/publications/3FA648ECECB88454C91804F09E2E56' target='_blank'>link</a>"), row.names = 1L, class = "data.frame")

# table setup -----------------------------------
cd <- list()
options <- list(dom = "t", ordering = FALSE, pageLength = 2, columnDefs = cd)
DT::datatable(table, options, selection = "none", rownames = FALSE, escape = FALSE)
```

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations that have comments on pubpeer (excluding Statcheck comments).

The PubPeer module uses the PubPeer API to check for each reference that has a DOI whether there are comments on the post-publication peer review platform. If comments are found, a link to the comments is provided. Comments by ‚ÄòStatcheck‚Äô on PubPeer are ignored, see https://retractionwatch.com/2016/09/02/heres-why-more-than-50000-psychology-studies-are-about-to-have-pubpeer-entries/.

The module requires that the reference has a DOI. If you run the doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

For more information, see [PubPeer](https://www.pubpeer.com/static/about).

This module was developed by Daniel Lakens and Lisa DeBruine

:::




