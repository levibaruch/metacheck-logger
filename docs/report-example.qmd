---
title: MetaCheck Report
subtitle: "To Err is Human: An Empirical Investigation"
author: "Daniel Lakens & Lisa DeBruine"
metacheck:
  version: "0.0.0.9066"
  report-created: "2026-01-25"
format:
  html:
    theme: 
      light: flatly
      dark: darkly
    respect-user-color-scheme: true
    toc: true
    toc-title: >
      <ul class="emoji-key">
        <li class="red">‚ö†Ô∏è possible problems detected;</li> 
        <li class="yellow">üîç something to check;</li> 
        <li class="green">‚úÖÔ∏è no problems detected;</li>  
        <li class="info">‚ÑπÔ∏è informational only;</li>  
        <li class="na">‚ö™Ô∏è not applicable;</li>  
        <li class="fail">‚ò†Ô∏è check failed</li>
      <ul>
      <hr/>
      **Table of Contents**
    toc-location: right
    toc-expand: 3
    page-layout: full
    title-block-banner: "#15342A"    #"#327118"
    title-block-banner-color: "white"
    link-external-newwindow: true
    embed-resources: true
    format-links: false
    smooth-scroll: true
execute: 
  echo: false
  error: true
---

<style>
  #TOC * { border-left: none; }
  #TOC > ul { border-left: 1px solid #15342A; }
  #TOC > ul > li > ul > li { text-indent: -1.2em; margin-left: 1.2em; }
  main h2, main h3 { margin-left: -20px; }
  main h2 { font-size: 150%; font-face: italics; }
  main h3 { font-size: 125%; }
  main h4 { font-size: 110%; }
  .emoji-key li { margin-bottom: 0.5em; }
  section::before { content: '' !important; }
  th { background-color: #555; color: white; }
  table.dataTable, .dataTables_scrollBody { margin: 0.5em 0; }
  #info { position: absolute; top: 2.5em; right: 2.5em; color: white; }
  #info a { color: white !important; }
  .quarto-dark tr.even { background-color: #333; }
  .quarto-dark tr.odd { background-color: #222; }
  .quarto-dark div.datatables { color: white; }
  .quarto-dark table.dataTable { border: 1px solid #555; }
  table strong { color: red; }
  .quarto-dark table strong { color: yellow; }
  summary { color: #333; }
  .quarto-dark summary { color: #CCC; }
</style>

::: {#info}
[MetaCheck](http://www.scienceverse.org/metacheck) version {{< meta metacheck.version >}}  
Report Created: {{< meta metacheck.report-created >}}  
 <!-- doi, if exists -->
:::

[Metacheck](https://www.scienceverse.org/metacheck/) is a tool that screens scientific manuscripts and aims to identify potential issues for improvement, thereby guiding researchers towards best practices. Metacheck is developed to help researchers correctly and completely report statistical results, automatically retrieve possible relevant information about citations, and improve how researchers share data, code, and preregistrations.

::: {.callout-tip title="Learn More" collapse="true"}

Metacheck combines existing and new checks in a module-based tool. It mainly relies on text search, retrieving information from external sources through API‚Äôs or web-scraping, but it also incorporates tool that use machine learning classifiers or large language models. The use of LLM‚Äôs is always optional. The development of Metacheck is guided by our [values statement](https://docs.google.com/document/d/1bbIkgUaiz3fpTXAeTF3h-gsfhqWEBN_n6OBcbFLfXjw/edit?tab=t.0). 

Metacheck often needs to balance false positives against false negatives, and prioritizes preventing false negatives. Like a spelling checker, which often highlights words that are not spelled incorrectly, Metacheck modules will contain false positives. We hope their rate is acceptable, given opportunities for improvement that Metacheck identifies. Modules are currently primarily validated on the psychological literature.

Metacheck is under continuous development. Issues can be submitted [on Github](https://github.com/scienceverse/metacheck/issues), and suggestions for improvement or feedback can be sent to D.Lakens@tue.nl.

:::


## Summary

- ‚ö†Ô∏è [COI Check](#coi-check){.red}: No conflict of interest statement was detected.  
- ‚ö†Ô∏è [Funding Check](#funding-check){.red}: No funding statement was detected.  
- ‚úÖÔ∏è [Open Practices Check](#open-practices-check){.green}: Shared data and code detected.  
- ‚ö†Ô∏è [Power Analysis Check](#power-analysis-check){.red}: We detected 2 potential power analyses.  
- ‚úÖÔ∏è [Randomization and Causal Claims](#randomization-and-causal-claims){.green}: 
    -  We identified 1 sentence describing randomization.
    -  No causal claims were observed in the title.
    -  No causal claims were observed in the abstract.  
- ‚ÑπÔ∏è [Preregistration Check](#preregistration-check){.info}: We found 2 preregistrations.  
- ‚ö†Ô∏è [Exact P-Values](#exact-p-values){.red}: We found 1 imprecise *p* value out of 3 detected.  
- ‚ö†Ô∏è [Marginal Significance](#marginal-significance){.red}: You described 2 effects with terms related to 'marginally significant'.  
- ‚ö†Ô∏è [Effect Sizes in t-tests and F-tests](#effect-sizes-in-t-tests-and-f-tests){.red}: We found 1 t-test and/or F-test where effect sizes are not reported.  
- ‚ö†Ô∏è [StatCheck](#statcheck){.red}: 1 possible error in t-tests or F-tests  
- üîç [Non-Significant P Value Check](#non-significant-p-value-check){.yellow}: We found 2 non-significant p values that should be checked for appropriate interpretation.  
- üîç [Code Check](#code-check){.yellow}: 
    -  We found 4 R, SAS, SPSS, or Stata code files in the 6 searched repositories.
    -  Some files loaded in the code were missing in the repository.
    -  Hardcoded file paths were found.
    -  Libraries/imports were loaded in multiple places.
    -  Some code files had no comments.
    -  We found 0 README files and 3 sources without READMEs: osf.io/cxjg4, https://github.com/Lakens/to_err_is_human, https://researchbox.org/4377.  
- üîç [DOI Check](#doi-check){.yellow}: We checked 1 reference in CrossRef and found 1 missing DOI.   
- üîç [Reference Accuracy](#reference-accuracy){.yellow}: We checked 4 references with DOIs in CrossRef and found  matches for 3.  
- ‚ÑπÔ∏è [Replication Check](#replication-check){.info}: You cited 1 article in the FReD replication database.  
- ‚ÑπÔ∏è [RetractionWatch](#retractionwatch){.info}: You cited 1 article in the RetractionWatch database.  
- ‚ÑπÔ∏è [Check PubPeer Comments](#check-pubpeer-comments){.info}: You cited 1 reference with comments in PubPeer.  
- ‚ÑπÔ∏è [Summarise References](#summarise-references){.info}: Summary information provided for 4 references  



## General Modules

### ‚ö†Ô∏è COI Check {#coi-check .red}

No conflict of interest statement was detected. Consider adding one.

::: {.callout-note title="How It Works" collapse="true"}

Identify and extract Conflicts of Interest (COI) statements.

The COI Check module uses regular expressions to check sentences for words related to conflict of interest statements. It will return the sentences in which the conflict of interest statement was found.

The function was inspired by [rtransparent](https://github.com/serghiou/rtransparent), which is no longer maintained. For their validation, see [the paper](https://doi.org/10.1371/journal.pbio.3001107).

Our version uses a more inclusive algorithm, which decreases false negatives (missing a potential COI) at the expense of increasing false positives (falsely detecting sentences as a COI statement).

This module was developed by Daniel Lakens

:::

### ‚ö†Ô∏è Funding Check {#funding-check .red}

No funding statement was detected. Consider adding one.

::: {.callout-note title="How It Works" collapse="true"}

Identify and extract funding statements.

The Funding Check module uses regular expressions to check sentences for words related to funding statements. It will return the sentences in which the conflict of interest statement was found.

The function was inspired by [rtransparent](https://github.com/serghiou/rtransparent), which is no longer maintained. For their validation, see [the paper](https://doi.org/10.1371/journal.pbio.3001107).

Our version uses a more inclusive algorithm, which decreases false negatives (missing a potential funding statement) at the expense of increasing false positives (falsely detecting sentences as a funding statement).

This module was developed by Daniel Lakens

:::

### ‚úÖÔ∏è Open Practices Check {#open-practices-check .green}

Shared data and code detected.

<details><summary>View detailed feedback</summary><div>

Data was openly shared for this article, based on the following text:

> Data is also available from <https://osf.io/5tbm9> and code is also available from <https://osf.io/629bx>.

Code was openly shared for this article, based on the following text:

> Data and analysis code is available on GitHub from <https://github.com/Lakens/to_err_is_human> and from <https://researchbox.org/4377>.

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module incorporates ODDPub into metacheck. ODDPub is a text mining algorithm that detects which publications disseminated Open Data or Open Code together with the publication.

The Open Practices Check runs Open Data Detection in Publications (ODDPub). ODDPub searches for text expressions that indicate that an article shared Open Data or Open Code together with the publication. More information on the package can be found at <https://github.com/quest-bih/oddpub>. The module only returns whether open data and code is found (the original package offers more fine-grained results). The tool was validated in the biomedical literature, see <https://osf.io/yv5rx/>.

ODDPub was developed by Nico Riedel, Vladislav Nachev, Miriam Kip, and Evgeny Bobrov at the QUEST Center for Transforming Biomedical Research, Berlin Institute of Health. <https://doi.org/10.5334/dsj-2020-042>

It might miss open data and code declarations when the words used in the manuscript are not in the pattern that ODDPub searches for, or when the repositories are not in the ODDpub code (e.g., ResearchBox).

This module was developed by Daniel Lakens

:::

## Method Modules

### ‚ö†Ô∏è Power Analysis Check {#power-analysis-check .red}

We detected 2 potential power analyses.

<details><summary>View detailed feedback</summary><div>

Some essential information could not be detected: sample_size, power, software

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(power_id = 1:2, power_type = c("sensitivity", 
"posthoc"), statistical_test = c("t-test", "t-test"), sample_size = c(50L, 
NA), alpha_level = c(0.05, 0.05), power = c(0.8, NA), effect_size = c(0.5, 
0.59), effect_size_metric = c("Cohen's d", "Cohen's d"), software = c("other", 
NA)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

You reported a power analysis that has been classified as 'post-hoc'. Calculating observed power is [almost never useful](https://lakens.github.io/statistical_inferences/08-samplesizejustification.html#sec-posthocpower). If you actually performed a sensitivity power analysis, label it as such explicitly.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(power_id = "1;2", text = "In this study we examine whether automated checks reduce the amount of errors that researchers make in scientific manuscripts. This study was preregistered at <https://osf.io/48ncu> and on AsPredicted at <https://aspredicted.org/by8i8v.pdf>. We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist. Scientists had the opportunity to make changes to their manuscript based on the feedback of the tool. We subsequently coded all manuscripts for mistakes, and counted the total number of mistakes. We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful). Data and analysis code is available on GitHub from <https://github.com/Lakens/to_err_is_human> and from <https://researchbox.org/4377>. Data is also available from <https://osf.io/5tbm9> and code is also available from <https://osf.io/629bx>. We conducted a <strong>sensitivity</strong> <strong>power</strong> analysis to determine that a Cohen's d of 0.50 is the smallest effect size that we could detect with 50 participants in each group and 80% <strong>power</strong>. On average researchers in the experimental (app) condition made fewer mistakes (M = 9.12) than researchers in the control (checklist) condition (M = 10.9), t(97.7) = 2.9, p = 0.005, d = 0.59."), row.names = c(NA, 
-1L), class = c("tbl_df", "tbl", "data.frame"))

# display table -----------------------------------
metacheck::report_table(table, "auto", 1, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

Power analyses need to contain the following information to be interpretable: the type of power analysis, the statistical test, the software used, sample size, critical alpha criterion, power level, effect size, and an effect size metric. In addition, it is recommended to make sure the power analysis is reproducible (by sharing the code, or a screenshot, of the power analysis), and to provide good arguments for why the study was designed to detect an effect of this size.

For an a-priori power analysis, where the sample size is determined, reporting all information would look like:

> An a priori power analysis for an independent samples t-test, conducted using the pwr.t.test function from pwr (Champely, 2020), indicated that for a Cohen's d = 0.5, an alpha level of 0.05, and a desired power level of 80% required at least 64 participants in each group.

For a sensitivity power analysis, this sentence would look like:

> A sensitivity power analysis for an independent samples t-test, conducted using the pwr.t.test function from pwr (Champely, 2020), indicated that with 64 participants in each group, and an alpha level of 0.05, a desired power level of 80% was reached for an effect size of d = 0.5.

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module uses a large language module (LLM) to extract information reported in power analyses, including the statistical test, sample size, alpha level, desired level of power,and magnitude and type of effect size.

If you have not set llm_use(TRUE) and supplied a groq API, the module will return paragraphs that potentially contain power analyses, based on a regular expression search.

The Power Analysis Check module uses regular expressions to identify sentences that contain a statistical power analysis. Without the use of an LMM, the module uses regular expressions to classify the power analysis as a-priori, sensitivity or post-hoc. With the use of an LMM, it checks if the power analysis is reported with all required information.

The regular expressions can miss power analyses, or fail to classify them correctly. The type of power analysis is often difficult to classify, which can easily be solved by explicitly specifying the type of power analysis as 'a-priori', 'sensitivity', or 'post-hoc'. Note that 'post-hoc' or 'observed' power is rarely useful. The LMM can fail to identify information in the paper, and will not have access to information in paragraphs in the paper other than those that contain the word 'power'. This package was validated by the Metacheck team on articles in Psychological Science.

This module was developed by Lisa DeBruine, Daniel Lakens and Cristian Mesquida

:::

### ‚úÖÔ∏è Randomization and Causal Claims {#randomization-and-causal-claims .green}

-  We identified 1 sentence describing randomization.
-  No causal claims were observed in the title.
-  No causal claims were observed in the abstract.

<details><summary>View detailed feedback</summary><div>

Journal Article Reporting Standards require details about randomization procedures, or how possible bias due to non-randomization is mitigated. This information is often not reported. Furthermore, researchers sometimes make causal claims that are not warranted, for example because there was no random assignment to conditions. This module checks how (non)randomization is reported, and checks for causal claims in the title and abstract. Researchers are asked to double check whether this information is reported completely and correctly.

#### Randomization

We identified 1 sentence describing randomization.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

If this was a study that contained random assignment to conditions, the journal article reporting standards (JARS) ask that you describe the following:

1. Random assignment method: Procedure used to generate the random assignment sequence, including details of any restriction (e.g., blocking, stratification)

2. Random assignment concealment: Whether sequence was concealed until interventions were assigned

3. Random assignment implementation: Who generated the assignment sequence, who enrolled participants, who assigned participants to groups

#### Causal Claims

No causal claims were observed in the title.

No causal claims were observed in the abstract.

::: {.callout-tip title="Learn More" collapse="true"}

For advice on how to make causal claims, and when not to, see:

Antonakis J, Bendahan S, Jacquart P, Lalive R (2010). &ldquo;On making causal claims: A review and recommendations.&rdquo; <em>The Leadership Quarterly</em>, <b>21</b>(6), 1086&ndash;1120. <a href="https://doi.org/10.1016/j.leaqua.2010.10.010">doi:10.1016/j.leaqua.2010.10.010</a>.

Grosz M, Rohrer J, Thoemmes F (2020). &ldquo;The Taboo Against Explicit Causal Inference in Nonexperimental Psychology.&rdquo; <em>Perspectives on Psychological Science</em>, <b>15</b>(5), 1243&ndash;1255. <a href="https://doi.org/10.1177/1745691620921521">doi:10.1177/1745691620921521</a>.

For the APA journal articles reporting standards, see <https://apastyle.apa.org/jars>

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

Aims to identify the presence of random assignment, and lists sentences that make causal claims in title or abstract.

The Randomization and Causal Claims Check first uses regular expressions to check whether the manuscript contains a statement about randomization to conditions. Subsequently, it sends the title and abstract to a [machine learning classifier developed by Rasoul Norouzi](https://github.com/rasoulnorouzi/causal_relation_miner) that runs on [HuggingFace](https://huggingface.co/spaces/lakens/causal_sentences). Causal statements are identified. Researchers are recommended to double check if causal statements are warranted, especially if no sentences describing randomization were detected.

The regular expressions can miss statements about randomization, or incorrectly assume there is a sentence describing randomization. The module can‚Äôt evaluate if the causal statements that are identified are warranted or not, and it only reminds users to double-check.

If you want to improve the detection of sentences describing randomization, or otherwise improve the module, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

### ‚ÑπÔ∏è Preregistration Check {#preregistration-check .info}

We found 2 preregistrations.

<details><summary>View detailed feedback</summary><div>

We found 2 preregistrations.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(id = c("<a href='https://osf.io/48ncu' target='_blank'>48ncu</a>", 
"<a href='https://aspredicted.org/by8i8v.pdf' target='_blank'>by8i8v</a>"
), title = c("Papercheck Test", "To err is human"), template = c("OSF Preregistration", 
"AsPredicted")), class = "data.frame", row.names = c(NA, -2L))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

Meta-scientific research has shown that deviations from preregistrations are often not reported or checked, and that the most common deviations concern the sample size. We recommend manually checking the full preregistration at the links below, and have provided the preregistered sample size.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(id = c("48ncu", "by8i8v"), sample_size = c("We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

::: {.callout-tip title="Full Preregistration" collapse="true"}

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Field = c("template_name", "title", "id", "link", 
"date_created", "date_modified", "date_registered", "embargo_end_date", 
"ia_url", "description", "study_type", "blinding", "study_design_overview", 
"data_collection_started", "existing_data_explanation", "data_collection_procedures", 
"sample_size", "sample_size_rationale", "stopping_rule", "design_independent_variables", 
"design_dependent_variables", "indices", "statistical_tests", 
"inference_criteria", "data_exclusion_criteria", "outliers_and_exclusions", 
"exploratory_analyses", "additional_comments", "research_questions"
), "Preregistration 1" = c("OSF Preregistration", "Papercheck Test", 
"48ncu", "https://osf.io/48ncu", "2025-11-28T19:14:24.639665", 
"2025-08-05T11:55:35.267689", "2025-11-28T19:14:24.608826", "NA", 
"https://archive.org/details/osf-registrations-48ncu-v1", "Automation can be used to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of metacheck to improve best practices.", 
"Experiment - A researcher randomly assigns treatments to study subjects, this includes field or lab experiments. This is also known as an intervention experiment and includes randomized controlled trials.", 
"No blinding is involved in this study. ", "Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
"Registration prior to creation of data", "", "We will include all scientists who are willing to participate. ", 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
"Based on an a-priori power analysis for an independent t-test, with an expected effect size of d = 0.6, an alpha of 0.05, and a desired power of 0.85, we needed to collect at least 50 participants per group. ", 
"We will stop after data from 100 researchers has been collected.", 
"Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
"We will code all manuscripts for mistakes, and count the total number of mistakes per manuscript.", 
"", "We will perform an independent t-test on the number of mistakes in each group.", 
"We will interpret effects as significant if p &lt; .05", "No exclusions are expected.", 
"No missing data is expected.\n", "We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful).", 
"", NA), "Preregistration 2" = c("AsPredicted", "To err is human", 
"by8i8v", "https://aspredicted.org/by8i8v.pdf", "2025/11/27 23:20 (PT)", 
NA, NA, NA, NA, NA, NA, NA, "Two conditions. In one, researchers receive automated feedback. In the control condition, they do not receive feedback.", 
NA, "No, no data have been collected for this study yet.", NA, 
"We randomly assigned 50 scientists to a condition where their manuscript was automatically checked for errors, and 50 scientists to a control condition with a checklist.", 
NA, NA, NA, "We will code all manuscripts for mistakes, and count the total number of mistakes per manuscript.", 
NA, "We will perform an independent t-test on the number of mistakes in each group.", 
NA, NA, "none are expected.", NA, "We also measured the expertise of researchers (in years) to explore whether the automated tool would be more useful, the less research experience researchers had. We also asked researchers to rate how useful they found the checklist or app on a scale from 1 (not at all) to 7 (extremely useful).", 
"Automation can be used to check for errors in scientific manuscripts, and inform authors about possible corrections. In this study we examine the usefulness of metacheck to improve best practices."
)), class = "data.frame", row.names = c("template_name", "title", 
"id", "link", "date_created", "date_modified", "date_registered", 
"embargo_end_date", "ia_url", "description", "study_type", "blinding", 
"study_design_overview", "data_collection_started", "existing_data_explanation", 
"data_collection_procedures", "sample_size", "sample_size_rationale", 
"stopping_rule", "design_independent_variables", "design_dependent_variables", 
"indices", "statistical_tests", "inference_criteria", "data_exclusion_criteria", 
"outliers_and_exclusions", "exploratory_analyses", "additional_comments", 
"research_questions"))

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

:::

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate of deviations from preregistrations, see:

van den Akker O, Bakker M, van Assen M, Pennington C, Verweij L, Elsherif M, Claesen A, Gaillard S, Yeung S, Frankenberger J, Krautter K, Cockcroft J, Kreuer K, Evans T, Heppel F, Schoch S, Korbmacher M, Yamada Y, Albayrak-Aydemir N, Wicherts J (2024). &ldquo;The potential of preregistration in psychology: Assessing preregistration producibility and preregistration-study consistency.&rdquo; <em>Psychological Methods</em>. <a href="https://doi.org/10.1037/met0000687">doi:10.1037/met0000687</a>.

For educational material on how to report deviations from preregistrations, see:

Lakens, Dani√´l (2024). &ldquo;When and How to Deviate From a Preregistration.&rdquo; <em>Collabra: Psychology</em>, <b>10</b>(1), 117094. <a href="https://doi.org/10.1525/collabra.117094">doi:10.1525/collabra.117094</a>.

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

Retrieve information from preregistrations in a standardised way,
and make them easier to check.

The Preregistration Check module identifies preregistrations on the OSF and AsPredicted based on links in the manuscript, retrieves the preregistration text, and organizes the information into a template. The module then uses regular expressions to identify text from AsPredicted, and the API to retrieve text from the OSF. The information in the preregistration is returned.

The module can‚Äôt extract information from non-structured preregistration templates (i.e., where the preregistration is uploaded in a single text field) and it can‚Äôt retrieve information in preregistrations that are stored as text documents on the OSF.

If you want to extend the package to be able to download information from other preregistration sites, reach out to the Metacheck development team.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

## Results Modules

### ‚ö†Ô∏è Exact P-Values {#exact-p-values .red}

We found 1 imprecise *p* value out of 3 detected.

<details><summary>View detailed feedback</summary><div>

Reporting *p* values imprecisely (e.g., *p* < .05) reduces transparency, reproducibility, and re-use (e.g., in *p* value meta-analyses). Best practice is to report exact p-values with three decimal places (e.g., *p* = .032) unless *p* values are smaller than 0.001, in which case you can use *p* < .001.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("P-Value" = "p > .05", Sentence = "There was no effect of experience on the reduction in errors when using the tool (p > .05), as the correlation was non-significant."), row.names = c(NA, 
-1L), class = c("tbl_df", "tbl", "data.frame"))

# display table -----------------------------------
metacheck::report_table(table, c(0.1, 0.9), 2, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

The APA manual states: Report exact *p* values (e.g., *p* = .031) to two or three decimal places. However, report *p* values less than .001 as *p* < .001. However, 2 decimals is too imprecise for many use-cases (e.g., a *p* value meta-analysis), so report *p* values with three digits.

American Psychological Association (2020). <em>Publication manual of the American Psychological Association</em>, 7 edition. American Psychological Association.

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

List any p-values reported with insufficient precision (e.g., p < .05 or p = n.s.)

This module uses regular expressions to identify p-values. It will flag any values reported as p > ? or p < numbers greater than .001.

We try to exclude figure and table notes like "* p < .05", but may not succeed at excluding all false positives.

This module was developed by Lisa DeBruine

:::

### ‚ö†Ô∏è Marginal Significance {#marginal-significance .red}

You described 2 effects with terms related to 'marginally significant'.

<details><summary>View detailed feedback</summary><div>

You described effects with terms related to 'marginally significant'. If *p* values above 0.05 are interpreted as an effect, you inflate the alpha level, and increase the Type 1 error rate. If a *p* value is higher than the prespecified alpha level, it should be interpreted as a non-significant result.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(section = c("abstract", "results"), text = c("The paper shows examples of (1) open and closed OSF links; (2a) citation of retracted papers, (2b) citations without a doi, (2c) citations with Pubpeer comments, (2d) citations in the FORTT replication database, and (2e) missing/mismatched/incorrect citations and references; (3a) R files with code on GitHub that do not load libraries in one location, (3b) load files that are not shared in the repository, (3c) lack comments, and (3d) have hard-coded files, (4) imprecise reporting of non-significant pvalues; (5) tests with and without effect sizes, (6) use of \"marginally significant\" to describe non-significant findings, and (7) retrieving information from preregistrations.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate at which non-significant p-values are interpreted as marginally significant, see:

Olsson-Collentine, A., van Assen, M. MAL, Hartgerink &amp;, J. CH (2019). &ldquo;The Prevalence of Marginally Significant Results in Psychology Over Time.&rdquo; <em>Psychological Science</em>, <b>30</b>, 576&ndash;586. <a href="https://doi.org/10.1177/0956797619830326">doi:10.1177/0956797619830326</a>.

For the list of terms used to identifify marginally significant results, see this [blog post by Matthew Hankins](https://web.archive.org/web/20251001114321/https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/).

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

List all sentences that describe an effect as 'marginally significant'.

The marginal module searches for regular expressions that match a predefined pattern. The list of terms is a subset of those listed in a [blog post by Matthew Hankins](https://web.archive.org/web/20251001114321/https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/). The module returns all sentences that match terms describing ‚Äòmarginally significant‚Äô results.

Some of the terms identified might not be problematic in some contexts, and there are ways to describe ‚Äòmarginal significance‚Äô that are not detected by the module.

This module was developed by Daniel Lakens

:::

### ‚ö†Ô∏è Effect Sizes in t-tests and F-tests {#effect-sizes-in-t-tests-and-f-tests .red}

We found 1 t-test and/or F-test where effect sizes are not reported.

<details><summary>View detailed feedback</summary><div>

We recommend checking the sentences below, and add any missing effect sizes.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating that effect sizes are often not reported:

* Peng, C.-Y. J., Chen, L.-T., Chiang, H.-M., & Chiang, Y.-C. (2013). The Impact of APA and AERA Guidelines on Effect Size Reporting. Educational Psychology Review, 25(2), 157‚Äì209. doi:[10.1007/s10648-013-9218-2](https://doi.org/10.1007/s10648-013-9218-2).

For educational material on reporting effect sizes:

* [Guide to Effect Sizes and Confidence Intervals](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals/)

:::

::: {.callout-tip title="All detected and assessed stats" collapse="true"}

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Sentence = c("On average researchers in the experimental (app) condition made fewer mistakes (M = 9.12) than researchers in the control (checklist) condition (M = 10.9), t(97.7) = 2.9, p = 0.005, d = 0.59.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
), Section = c("method", "results"), "Effect Size" = c("d = 0.59", 
NA), "Reported Test" = c("t(97.7) = 2.9", "t(97.2) = -1.96"), 
    "Test Type" = c("t-test", "t-test")), row.names = c(NA, -2L
), class = c("tbl_df", "tbl", "data.frame"))

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

The Effect Size module checks for effect sizes in t-tests and F-tests.

The Effect Size check searches for regular expressions that match a predefined pattern. The module was validated on APA reported statistical tests, and might miss effect sizes that were reported in other reporting styles. It was validated by the Metacheck team on papers published in Psychological Science.

If you want to extend the package to detect effect sizes for additional tests, reach out to the Metacheck development team.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### ‚ö†Ô∏è StatCheck {#statcheck .red}

1 possible error in t-tests or F-tests

<details><summary>View detailed feedback</summary><div>

We detected possible errors in test statistics. Note that as the accuracy of statcheck has only been validated for *t*-tests and *F*-tests. As Metacheck only uses validated modules, we only provide statcheck results for *t* tests and *F*-tests.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Text = "t(97.2) = -1.96, p = 0.152", "Recomputed p" = 0.05286, 
    Section = "results", Sentence = "On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."), row.names = 2L, class = c("statcheck", 
"data.frame"))

# display table -----------------------------------
metacheck::report_table(table, c("10em", NA, NA, NA), 2, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific research on the validity of statcheck, and it's usefulness to prevent statistical reporting errors, see:

Nuijten M, van Assen M, Hartgerink C, Epskamp S, Wicherts J (2017). &ldquo;The validity of the tool &quot;statcheck&quot; in discovering statistical reporting inconsistencies.&rdquo; <a href="https://doi.org/10.31234/osf.io/tcxaja">doi:10.31234/osf.io/tcxaja</a>, Preprint.

Nuijten M, Wicherts J (2023). &ldquo;The effectiveness of implementing statcheck in the peer review process to avoid statistical reporting errors.&rdquo; <a href="https://doi.org/10.31234/osf.io/bxau9">doi:10.31234/osf.io/bxau9</a>, Preprint.

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

Check consistency of p-values and test statistics

The Statcheck module runs Statcheck. Statcheck searches for regular expressions that match a predefined pattern, and identifies APA reported statistical tests. More information on the package can be found at <https://github.com/cran/statcheck>. The module only returns Statcheck results for t-tests and F-tests, as these are the only tests which have been validated, see <https://osf.io/preprints/psyarxiv/tcxaj_v1/>.

Statcheck was developed by Mich√®le Nuijten and Sascha Epskamp.

Statcheck considers p = 0.000 an error, as you should report p < 0.001. Furthermore, p < 0.03 is an error if the p-value was 0.031, and one should simply report exact p-values (p = 0.031). Statcheck might miss one-sided tests, and falsely assume the p-value is incorrect. For more information, see [StatCheck](https://statcheck.io/).

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### üîç Non-Significant P Value Check {#non-significant-p-value-check .yellow}

We found 2 non-significant p values that should be checked for appropriate interpretation.

<details><summary>View detailed feedback</summary><div>

Meta-scientific research has shown nonsignificant p values are commonly misinterpreted. It is incorrect to infer that there is 'no effect', 'no difference', or that groups are 'the same' after p > 0.05.

It is possible that there is a true non-zero effect, but that the study did not detect it. Make sure your inference acknowledges that it is possible that there is a non-zero effect. It is correct to include the effect is 'not significantly' different, although this just restates that p > 0.05.

Metacheck does not yet analyze automatically whether sentences which include non-significant p-values are correct, but we recommend manually checking the sentences below for possible misinterpreted non-significant p values.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Text = c("p = 0.152", "p > .05"), Sentence = c("On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152.", 
"There was no effect of experience on the reduction in errors when using the tool (p > .05), as the correlation was non-significant."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# display table -----------------------------------
metacheck::report_table(table, c(0.1, 0.9), 2, FALSE)
```

::: {.callout-tip title="Learn More" collapse="true"}

For metascientific articles demonstrating the rate of misinterpretations of non-significant results is high, see:

Aczel B, Palfi B, Szollosi A, Kovacs M, Szaszi B, Szecsi P, Zrubka M, Gronau Q, van den Bergh D, Wagenmakers E (2018). &ldquo;Quantifying Support for the Null Hypothesis in Psychology: An Empirical Investigation.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>(3), 357&ndash;366. <a href="https://doi.org/10.1177/2515245918773742">doi:10.1177/2515245918773742</a>.

Murphy S, Merz R, Reimann L, Fern√°ndez A (2025). &ldquo;Nonsignificance misinterpreted as an effect‚Äôs absence in psychology: Prevalence and temporal analyses.&rdquo; <em>Royal Society Open Science</em>, <b>12</b>(3), 242167. <a href="https://doi.org/10.1098/rsos.242167">doi:10.1098/rsos.242167</a>.

For educational material on preventing the misinterpretation of p values, see [Improving Your Statistical Inferences](https://lakens.github.io/statistical_inferences/01-pvalue.html#sec-misconception1).

:::

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks for imprecisely reported p values. If p > .05 is detected, it warns for misinterpretations.

The nonsignificant p-value check searches for regular expressions that match a predefined pattern. The module identifies all p-values in a manuscript and selects those that are not reported to be smaller than or equal to 0.05. It returns all sentences containing non-significant p-values.

In the future, the Metacheck team aims to incorporate a machine learning classifier to only return sentences likely to contain misinterpretations. If you want to help to improve the module, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

### üîç Code Check {#code-check .yellow}

-  We found 4 R, SAS, SPSS, or Stata code files in the 6 searched repositories.
-  Some files loaded in the code were missing in the repository.
-  Hardcoded file paths were found.
-  Libraries/imports were loaded in multiple places.
-  Some code files had no comments.
-  We found 0 README files and 3 sources without READMEs: osf.io/cxjg4, https://github.com/Lakens/to_err_is_human, https://researchbox.org/4377.

<details><summary>View detailed feedback</summary><div>

Below, we describe some best coding practices and give the results of automatic evaluation of these practices in the code files below. This check may miss things or produce false positives if your scripts are less typical.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Code File Name" = c("bad.R", "bad.Rmd", "analysis.R", 
"Code/Study 1.r", "Codebook/Study 1 - csv - dataset 1.csv___CODEBOOK.csv", 
"Codebook/Study 1.csv___CODEBOOK.csv", "Data/Study 1 - csv - dataset 1.csv", 
"Data/Study 1.csv", "Materials/Study 1 - test txt file.txt", 
"Table of Contents Box #4377.csv"), Download = c("<a href='https://osf.io/download/6935bc117049c887a125cb25/'>download</a>", 
"<a href='https://osf.io/download/uq347/'>download</a>", "<a href='https://raw.githubusercontent.com/Lakens/to_err_is_human/main/analysis.R'>download</a>", 
"<a href='https://researchbox.org/4377'>download</a>", "<a href='https://researchbox.org/4377'>download</a>", 
"<a href='https://researchbox.org/4377'>download</a>", "<a href='https://researchbox.org/4377'>download</a>", 
"<a href='https://researchbox.org/4377'>download</a>", "<a href='https://researchbox.org/4377'>download</a>", 
"<a href='https://researchbox.org/4377'>download</a>")), row.names = c(1L, 
3L, 4L, 6L, 7L, 8L, 9L, 10L, 11L, 12L), class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

#### Missing Files

The scripts load files, but 1 scripts loaded files that could not be automatically identified in the repository. Check if the following files are made available, so that others can reproduce your code, or that the files are missing:

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Code File name" = "Code/Study 1.r", Language = "R", 
    "Files loaded in code but missing in repository" = "file.csv"), row.names = 4L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

#### Hardcoded Paths

Best programming practice is to use relative file paths instead of hardcoded file paths (e.g., C://Lakens/files) as these folder names do not exist on other computers. The following hardcoded file paths were found in 4 code file(s).

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Code File name" = c("bad.R", "bad.Rmd", "analysis.R", 
"Code/Study 1.r"), Language = c("R", "R", "R", "R"), "Absolute paths found" = c("data <- read_csv(\"C://Lakens/files/example.csv\")", 
"data <- read_csv(\"C://Lakens/files/example.csv\"), data2 <- read_csv(\"C://Lakens/files/example2.csv\")", 
"data_1 <- read.csv(\"C:/path/to/a/file.csv\")", "abs_path_pc <- read.csv(\"C:/lisa/file.csv\")"
)), row.names = c(NA, 4L), class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

#### Libraries / Imports

Best programming practice is to load all required libraries/imports in one block near the top of the code. In 3 code files, libraries/imports were at multiple places (i.e., with more than 3 lines in between). This was true in the following files, where libraries/imports were loaded on the following lines:

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Code File name" = c("bad.R", "bad.Rmd", "analysis.R"
), Language = c("R", "R", "R"), "Lines at which libraries/imports are loaded" = c("1, 13", 
"6, 25", "1, 11")), row.names = c(NA, 3L), class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

#### Code Comments

Best programming practice is to add comments to code, to explain what the code does (to yourself in the future, or peers who want to re-use your code).

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Code File name" = c("bad.R", "bad.Rmd", "analysis.R", 
"Code/Study 1.r"), Language = c("R", "R", "R", "R"), "Percentage of lines that are comments" = c("5%", 
"2%", "0%", "11%")), row.names = c(NA, 4L), class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 5, FALSE)
```

#### README

We found 0 README files and 3 sources without READMEs: osf.io/cxjg4, https://github.com/Lakens/to_err_is_human, https://researchbox.org/4377. 

README files are a way to document the contents and structure of a folder, helping users locate the information they need. You can use a README to document changes to a repository, and explain how files are named. Please consider adding a README to each repository.

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module retrieves information from repositories (OSF and GitHub) about code files (R, SAS, SPSS, Stata), zip files, and readme.

The Code Check module lists files on the OSF and GitHub based on links in the manuscript, and retrieves R, Rmd, Qmd, SAS, SPSS, and Stata files. The module then uses regular expressions to check the code. The regular expression search will detect the number of comments, the lines at which libraries/imports are loaded, attempts to detect absolute paths to files, and lists files that are loaded, and checks if these files are in the repository. It will also check for a readme file in the repository, and will warn it can‚Äôt examine the contents of zip files. The module will return suggestions to improve the code if there are no comments, if libraries/imports are loaded in lines further than 4 lines apart, if files that are loaded are not in the repository, and if hardcoded file paths are found.

The regular expressions can miss information in code files, or falsely detect parts of the code as a fixed file path. Libraries/imports might be loaded in one block, even if there are more than 3 intermittent lines. The package was validated internally on papers published in Psychological Science. There might be valid reasons why some loaded files can‚Äôt be shared, but the module can‚Äôt evaluate these reasons, and always gives a warning.

If you want to extend the package to be able to download files from additional data repositories, or perform additional checks on code files, or make the checks work on other types of code files, reach out to the Metacheck development team.

This module was developed by Daniel Lakens

:::

## Reference Modules

### üîç DOI Check {#doi-check .yellow}

We checked 1 reference in CrossRef and found 1 missing DOI. 

<details><summary>View detailed feedback</summary><div>

Double check any references listed in the tables below. The match score gives an indication of how good the match was. Many books do not have a DOI or are not listed in CrossRef. Garbled references are usually a result of poor parsing of the paper by grobid; we are working on more accurate alternatives.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Found DOI" = "<a href='https://doi.org/10.1177/2515245918770963' target='_blank'>doi.org/10.1177/2515245918770963</a>", 
    "Match Score" = 69, "Original Reference" = "Lakens D (2018). &ldquo;Equivalence testing for psychological research.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>, 259-270."), row.names = 1L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks references for missing DOIs or DOIs with an invalid format.

This module works by identifying references that do not have a DOI or have a DOI that does not have a valid format. It then looks up these references by title, author and journal or book title in CrossRef. A DOI match is returned for the reference with the highest match above the `crossref_min_score` (default of 50).

Carefully check the returned results, as problems with reference import can lead to false positives.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### üîç Reference Accuracy {#reference-accuracy .yellow}

We checked 4 references with DOIs in CrossRef and found  matches for 3.

<details><summary>View detailed feedback</summary><div>

Double check any references listed in the tables below. This tool has a high false positive rate.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Unfound Reference" = "Smith F (2021). &ldquo;Human error is a symptom of a poor design.&rdquo; <em>Journal of Journals</em>, <b>0</b>(0), 0. <a href=\"https://doi.org/10.0000/0123456789\">doi:10.0000/0123456789</a>."), row.names = 3L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, 5, 2, FALSE)
```

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Original Title" = "Evil Genius? How Dishonesty Can Lead to Greater Creativity", 
    "CrossRef Title" = "Retracted: Evil Genius? How Dishonesty Can Lead to Greater Creativity", 
    Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>."), row.names = 2L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, 5, 2, FALSE)
```

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list("Original Authors" = "D Lakens", "CrossRef Authors" = "D Lakens, A Scheel, P Isager", 
    Reference = "Lakens D (2018). &ldquo;Equivalence testing for psychological research.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>, 259-270."), class = "data.frame", row.names = 4L)

# display table -----------------------------------
metacheck::report_table(table, 5, 2, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks references for mismatches with CrossRef.

It looks up the DOIs originally present in your paper (not those found by ref_doi_check) and returns the bibliographic information.

We then check that the title from your reference section is the same as the retrieved title (ignoring differences in capitalisation) and that all author last names in your reference section are also in the retrieved author list (we do not check first names or order yet). This check is done for all references with crossref entries, including those found by ref_doi_check, if it was previously run.

Mismatches may be because of problems with our parsing of references from your PDF (we're working on improving this), incorrect formatting in CrossRef, or minor differences in punctuation.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### ‚ÑπÔ∏è Replication Check {#replication-check .info}

You cited 1 article in the FReD replication database.

<details><summary>View detailed feedback</summary><div>

We checked 4 references with DOIs. You cited 1 article in the FReD replication database.

Check if you are aware of the replication studies, and cite them where appropriate.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Reference = "Gangestad SW, Thornhill R (1998). &ldquo;Menstrual cycle variation in women's preferences for the scent of symmetrical men.&rdquo; <em>Proceedings Biological Sciences</em>, <b>22</b>, 927-933. <a href=\"https://doi.org/10.1098/rspb.1998.0380\">doi:10.1098/rspb.1998.0380</a>.", 
    "Replication Reference" = "Jones, B. C., Hahn, A. C., Fisher, C. I., Wang, H., Kandrik, M., Han, C., Fasolt, V., Morrison, D., Lee, A. J., Holzleitner, I. J., O‚ÄôShea, K. J., Roberts, S. C., Little, A. C., & DeBruine, L. M. (2018). No Compelling Evidence that Preferences for Facial Masculinity Track Changes in Women‚Äôs Hormonal Status. Psychological Science, 29(6), 996-1005. https://doi.org/10.1177/0956797618760197 <a href='https://doi.org/10.1177/0956797618760197' target='_blank'>doi.org/10.1177/0956797618760197</a>"), row.names = 1L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, c(0.5, 0.5), 2, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations of original studies for which replication studies exist in the Replication Database.

The Replication Check module compares the reference list against studies in the FORRT replication database based on the DOI. If a study in the database is found, a reminder is provided that a replication of the original study exists, and should be cited (currently, a warning is provided regardless of whether the replication study is already cited).

The module requires that the reference has a DOI. If you run the ref_doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

It is possible the original study was cited for other reasons than the empirical claim tested, or that the replication in the FORRT replication database is for only one of the studies in the paper, and not the study the authors discuss.

The database can be manually updated with the `FReD_update()`` function. For more information, see <https://forrt.org/FReD/>.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### ‚ÑπÔ∏è RetractionWatch {#retractionwatch .info}

You cited 1 article in the RetractionWatch database.

<details><summary>View detailed feedback</summary><div>

We checked 4 references with DOIs. You cited 1 article in the RetractionWatch database.

Check if you are aware of the replication studies, and cite them where appropriate.

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>.", 
    "RW Type" = "Retraction"), row.names = c(NA, -1L), class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations in the RetractionWatch Database.

The RetractionWatch Check module compares the reference list against studies in the RetractionWatch database based on the DOI. If a study in the database is found, a reminder is provided that the study was retracted, has an expression of concern, or a correction.

The module requires that the reference has a DOI. If you run the ref_doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

It is possible the authors are already aware that a study was retracted, but the module can't evaluate this.

The database can be manually updated with the rw_update function. For more information, see https://gitlab.com/crossref/retraction-watch-data.

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### ‚ÑπÔ∏è Check PubPeer Comments {#check-pubpeer-comments .info}

You cited 1 reference with comments in PubPeer.

<details><summary>View detailed feedback</summary><div>

We checked 4 references with DOIs. You cited 1 reference with comments in PubPeer.

Pubpeer is a platform for post-publication peer review. We have filtered out Pubpeer comments by 'Statcheck'. You can check out the comments by visiting the URLs below:

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(Reference = "Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>.", 
    Comments = 3, "PubPeer Link" = "<a href='https://pubpeer.com/publications/3FA648ECECB88454C91804F09E2E56' target='_blank'>link</a>"), row.names = 1L, class = "data.frame")

# display table -----------------------------------
metacheck::report_table(table, "auto", 2, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

This module checks references and warns for citations that have comments on pubpeer (excluding Statcheck comments).

The PubPeer module uses the PubPeer API to check for each reference that has a DOI whether there are comments on the post-publication peer review platform. If comments are found, a link to the comments is provided. Comments by ‚ÄòStatcheck‚Äô on PubPeer are ignored, see https://retractionwatch.com/2016/09/02/heres-why-more-than-50000-psychology-studies-are-about-to-have-pubpeer-entries/.

The module requires that the reference has a DOI. If you run the doi_check module in a pipeline before this, it will use the enhanced DOI list from that module, otherwise it will only run on references with existing DOIs.

For more information, see [PubPeer](https://www.pubpeer.com/static/about).

This module was developed by Daniel Lakens and Lisa DeBruine

:::

### ‚ÑπÔ∏è Summarise References {#summarise-references .info}

Summary information provided for 4 references

<details><summary>View detailed feedback</summary><div>

```{r}
#| echo: false

# table data --------------------------------------
table <- structure(list(ref = c("Gangestad SW, Thornhill R (1998). &ldquo;Menstrual cycle variation in women's preferences for the scent of symmetrical men.&rdquo; <em>Proceedings Biological Sciences</em>, <b>22</b>, 927-933. <a href=\"https://doi.org/10.1098/rspb.1998.0380\">doi:10.1098/rspb.1998.0380</a>.", 
"Gino F, Wiltermuth SS (2014). &ldquo;Evil Genius? How Dishonesty Can Lead to Greater Creativity.&rdquo; <em>Psychological Science</em>, <b>25</b>(4), 973-981. <a href=\"https://doi.org/10.1177/0956797614520714\">doi:10.1177/0956797614520714</a>.", 
"Smith F (2021). &ldquo;Human error is a symptom of a poor design.&rdquo; <em>Journal of Journals</em>, <b>0</b>(0), 0. <a href=\"https://doi.org/10.0000/0123456789\">doi:10.0000/0123456789</a>.", 
"Lakens D (2018). &ldquo;Equivalence testing for psychological research.&rdquo; <em>Advances in Methods and Practices in Psychological Science</em>, <b>1</b>, 259-270."
), crossref_DOI = c("10.1098/rspb.1998.0380", "10.1177/0956797614520714", 
"10.0000/0123456789", "10.1177/2515245918770963"), crossref_doi_found = c(NA, 
NA, NA, TRUE), crossref_ref_not_found = c(FALSE, FALSE, TRUE, 
FALSE), crossref_title_mismatch = c(FALSE, TRUE, FALSE, FALSE
), crossref_author_mismatch = c(FALSE, FALSE, FALSE, TRUE), pubpeer_total_comments = c(NA, 
3, NA, NA), pubpeer_url = c(NA, "https://pubpeer.com/publications/3FA648ECECB88454C91804F09E2E56", 
NA, NA), pubpeer_users = c(NA, "Statcheck , Aioliops Novaeguineae, Hoya Camphorifolia", 
NA, NA), replication_ref = c("Jones, B. C., Hahn, A. C., Fisher, C. I., Wang, H., Kandrik, M., Han, C., Fasolt, V., Morrison, D., Lee, A. J., Holzleitner, I. J., O‚ÄôShea, K. J., Roberts, S. C., Little, A. C., & DeBruine, L. M. (2018). No Compelling Evidence that Preferences for Facial Masculinity Track Changes in Women‚Äôs Hormonal Status. Psychological Science, 29(6), 996-1005. https://doi.org/10.1177/0956797618760197", 
NA, NA, NA), replication_doi = c("10.1177/0956797618760197", 
NA, NA, NA), retractionwatch = c(NA, "Retraction", NA, NA)), class = "data.frame", row.names = c(NA, 
-4L))

# display table -----------------------------------
metacheck::report_table(table, "auto", 10, FALSE)
```

</div></details>

::: {.callout-note title="How It Works" collapse="true"}

Summarise information about each reference in a paper.

This module summarises previously-run reference section modules: ref_doi_check, ref_accuracy, ref_pubpeer, ref_replication, and ref_retractiuon.

This module was developed by Lisa DeBruine

:::




